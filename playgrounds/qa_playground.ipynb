{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: SQLAlchemy\r\n",
      "Version: 1.4.52\r\n",
      "Summary: Database Abstraction Library\r\n",
      "Home-page: https://www.sqlalchemy.org\r\n",
      "Author: Mike Bayer\r\n",
      "Author-email: mike_mp@zzzcomputing.com\r\n",
      "License: MIT\r\n",
      "Location: /usr/local/lib/python3.10/dist-packages\r\n",
      "Requires: greenlet\r\n",
      "Required-by: fastapi-utils, langchain, langchain-community, sqlalchemy-cockroachdb, sqlmodel\r\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import io\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# from langchain import retrievers\n",
    "from langchain_community.storage.redis import RedisStore\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from langchain_core.messages import HumanMessage, BaseMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from litellm import Router\n",
    "\n",
    "from apps.inners.models.dtos.element_category import ElementCategory\n",
    "from apps.outers.exceptions import use_case_exception\n",
    "\n",
    "os.chdir(\"/app\")\n",
    "from uuid import UUID\n",
    "\n",
    "from IPython.lib.display import IFrame\n",
    "from sqlmodel.ext.asyncio.session import AsyncSession\n",
    "from starlette.datastructures import State\n",
    "\n",
    "from apps.inners.models.daos.document import Document\n",
    "from apps.inners.models.dtos.contracts.responses.managements.documents.file_document_response import \\\n",
    "    FileDocumentResponse\n",
    "from apps.inners.models.dtos.contracts.responses.managements.documents.text_document_response import \\\n",
    "    TextDocumentResponse\n",
    "from apps.inners.models.dtos.contracts.responses.managements.documents.web_document_response import WebDocumentResponse\n",
    "from apps.inners.use_cases.managements.document_management import DocumentManagement\n",
    "from apps.inners.use_cases.managements.file_document_management import FileDocumentManagement\n",
    "from apps.inners.use_cases.managements.text_document_management import TextDocumentManagement\n",
    "from apps.inners.use_cases.managements.web_document_management import WebDocumentManagement\n",
    "from typing import List\n",
    "from typing import TypedDict, Tuple, Dict, Any\n",
    "\n",
    "import dotenv\n",
    "import litellm\n",
    "from datasets import load_dataset\n",
    "from dotenv import find_dotenv\n",
    "from langchain_community.chat_models import ChatLiteLLMRouter\n",
    "from langchain_community.vectorstores.milvus import Milvus\n",
    "from langchain_core.runnables.base import RunnableLike, RunnableSerializable\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.graph import CompiledGraph\n",
    "from ragas import evaluate\n",
    "from unstructured.documents.elements import Element, Table, Image, Text\n",
    "from unstructured.partition.auto import partition\n",
    "from unstructured.partition.utils.constants import PartitionStrategy\n",
    "\n",
    "from apps.inners.use_cases.embeddings.hugging_face_e5_instruct_embedding import HuggingFaceE5InstructEmbeddings\n",
    "from apps.outers.datastores.four_datastore import FourDatastore\n",
    "from apps.outers.datastores.one_datastore import OneDatastore\n",
    "from apps.outers.datastores.three_datastore import ThreeDatastore\n",
    "from apps.outers.datastores.two_datastore import TwoDatastore\n",
    "from apps.outers.repositories.file_document_repository import FileDocumentRepository\n",
    "from apps.outers.repositories.text_document_repository import TextDocumentRepository\n",
    "from apps.outers.repositories.web_document_repository import WebDocumentRepository\n",
    "from tests.containers.test_container import TestContainer\n",
    "from tests.seeders.all_seeder import AllSeeder\n",
    "\n",
    "!pip show sqlalchemy\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T06:07:01.484603Z",
     "start_time": "2024-03-31T06:06:58.991562Z"
    }
   },
   "id": "ccc0387222ff1e05",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# import tensorflow\n",
    "# \n",
    "# tensorflow.config.list_physical_devices('GPU')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "790b046202087813",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# import torch\n",
    "# \n",
    "# torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T06:00:26.567057Z",
     "start_time": "2024-03-31T06:00:26.564675Z"
    }
   },
   "id": "8638b0a2a9038acf",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dotenv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mdotenv\u001B[49m\u001B[38;5;241m.\u001B[39mload_dotenv(find_dotenv())\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mFrame\u001B[39m(src):\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m IFrame(src, width\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m700\u001B[39m, height\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m500\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'dotenv' is not defined"
     ]
    }
   ],
   "source": [
    "dotenv.load_dotenv(find_dotenv())\n",
    "\n",
    "\n",
    "def Frame(src):\n",
    "    return IFrame(src, width=700, height=500)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T06:00:27.021547Z",
     "start_time": "2024-03-31T06:00:27.009363Z"
    }
   },
   "id": "f0a206c2ced40865",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test_container = TestContainer()\n",
    "\n",
    "one_datastore: OneDatastore = test_container.applications.datastores.one()\n",
    "two_datastore: TwoDatastore = test_container.applications.datastores.two()\n",
    "three_datastore: ThreeDatastore = test_container.applications.datastores.three()\n",
    "four_datastore: FourDatastore = test_container.applications.datastores.four()\n",
    "temp_datastore: ThreeDatastore = test_container.applications.datastores.temp()\n",
    "\n",
    "file_document_repository: FileDocumentRepository = test_container.applications.repositories.file_document()\n",
    "text_document_repository: TextDocumentRepository = test_container.applications.repositories.text_document()\n",
    "web_document_repository: WebDocumentRepository = test_container.applications.repositories.web_document()\n",
    "\n",
    "document_management: DocumentManagement = test_container.applications.use_cases.managements.document()\n",
    "file_document_management: FileDocumentManagement = test_container.applications.use_cases.managements.file_document()\n",
    "text_document_management: TextDocumentManagement = test_container.applications.use_cases.managements.text_document()\n",
    "web_document_management: WebDocumentManagement = test_container.applications.use_cases.managements.web_document()\n",
    "\n",
    "all_seeder: AllSeeder = test_container.seeders.all()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T05:19:32.749261Z",
     "start_time": "2024-03-31T05:19:32.690210Z"
    }
   },
   "id": "8eb9c2d30510de79",
   "execution_count": 64
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "await all_seeder.up()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T05:19:34.238160Z",
     "start_time": "2024-03-31T05:19:33.654071Z"
    }
   },
   "id": "1cf8b24205467927",
   "execution_count": 65
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "await all_seeder.down()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T22:34:52.675058Z",
     "start_time": "2024-03-27T22:34:52.388417Z"
    }
   },
   "id": "d18db5f9358936e",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await two_datastore.client.set(\"test\", \"test\", ex=10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T21:50:48.726484Z",
     "start_time": "2024-03-22T21:50:48.718837Z"
    }
   },
   "id": "bf6d7b9a59665490",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# loading the V2 dataset\n",
    "amnesty_qa = load_dataset(\"explodinggradients/amnesty_qa\", \"english_v2\", trust_remote_code=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T18:45:05.238585Z",
     "start_time": "2024-03-28T18:45:00.627771Z"
    }
   },
   "id": "ec00454c2bc21e24",
   "execution_count": 118
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    eval: Dataset({\n        features: ['question', 'ground_truth', 'answer', 'contexts'],\n        num_rows: 20\n    })\n})"
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amnesty_qa"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T18:45:05.243175Z",
     "start_time": "2024-03-28T18:45:05.239851Z"
    }
   },
   "id": "f11930c0068cc28f",
   "execution_count": 119
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class PartitionDocumentProcessor:\n",
    "    def __init__(\n",
    "            self,\n",
    "            document_management: DocumentManagement,\n",
    "            file_document_management: FileDocumentManagement,\n",
    "            text_document_management: TextDocumentManagement,\n",
    "            web_document_management: WebDocumentManagement,\n",
    "    ):\n",
    "        self.document_management = document_management\n",
    "        self.file_document_management = file_document_management\n",
    "        self.text_document_management = text_document_management\n",
    "        self.web_document_management = web_document_management\n",
    "\n",
    "    async def _partition_file(self, state: State, found_document: Document) -> List[Element]:\n",
    "        found_file_document: FileDocumentResponse = await self.file_document_management.find_one_by_id(\n",
    "            state=state,\n",
    "            id=found_document.id\n",
    "        )\n",
    "        file_data: bytes = self.file_document_management.file_document_repository.get_object_data(\n",
    "            object_name=found_file_document.file_name\n",
    "        )\n",
    "        extract_image_path: Path = self.file_document_management.file_document_repository.file_path / found_file_document.file_data_hash\n",
    "        extract_image_path.mkdir(exist_ok=True)\n",
    "        shutil.rmtree(extract_image_path)\n",
    "        elements = partition(\n",
    "            metadata_filename=found_file_document.file_name,\n",
    "            file=io.BytesIO(file_data),\n",
    "            extract_images_in_pdf=True,\n",
    "            extract_image_block_output_dir=str(extract_image_path),\n",
    "            strategy=PartitionStrategy.HI_RES,\n",
    "            hi_res_model_name=\"yolox\"\n",
    "        )\n",
    "\n",
    "        return elements\n",
    "\n",
    "    async def _partition_text(self, state: State, found_document: Document) -> List[Element]:\n",
    "        found_text_document: TextDocumentResponse = await self.text_document_management.find_one_by_id(\n",
    "            state=state,\n",
    "            id=found_document.id\n",
    "        )\n",
    "        elements = partition(\n",
    "            text=found_text_document.text_content\n",
    "        )\n",
    "\n",
    "        return elements\n",
    "\n",
    "    async def _partition_web(self, state: State, found_document: Document) -> List[Element]:\n",
    "        found_web_document: WebDocumentResponse = await self.web_document_management.find_one_by_id(\n",
    "            state=state,\n",
    "            id=found_document.id\n",
    "        )\n",
    "        elements = partition(\n",
    "            url=found_web_document.web_url\n",
    "        )\n",
    "\n",
    "        return elements\n",
    "\n",
    "    async def partition(self, state: State, document_id: UUID) -> List[Element]:\n",
    "        found_document: Document = await self.document_management.find_one_by_id(\n",
    "            state=state,\n",
    "            id=document_id\n",
    "        )\n",
    "        if found_document.document_type_id == \"file\":\n",
    "            elements: List[Element] = await self._partition_file(\n",
    "                state=state,\n",
    "                found_document=found_document\n",
    "            )\n",
    "        elif found_document.document_type_id == \"text\":\n",
    "            elements: List[Element] = await self._partition_text(\n",
    "                state=state,\n",
    "                found_document=found_document\n",
    "            )\n",
    "        elif found_document.document_type_id == \"web\":\n",
    "            elements: List[Element] = await self._partition_web(\n",
    "                state=state,\n",
    "                found_document=found_document\n",
    "            )\n",
    "        else:\n",
    "            raise use_case_exception.DocumentTypeNotSupported()\n",
    "\n",
    "        return elements\n",
    "\n",
    "\n",
    "class CategoryDocumentProcessor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    async def categorize_elements(self, elements: List[Element]) -> ElementCategory:\n",
    "        element_category: ElementCategory = ElementCategory(\n",
    "            texts=[],\n",
    "            tables=[],\n",
    "            images=[]\n",
    "        )\n",
    "\n",
    "        for element in elements:\n",
    "            if any(\n",
    "                    element_type in str(type(element)) for element_type in\n",
    "                    [\"unstructured.documents.elements.Text\", \"unstructured.documents.elements.NarrativeText\"]\n",
    "            ):\n",
    "                element_category.texts.append(element)\n",
    "            elif any(\n",
    "                    element_type in str(type(element)) for element_type in\n",
    "                    [\"unstructured.documents.elements.Table\"]\n",
    "            ):\n",
    "                element_category.tables.append(element)\n",
    "            elif any(\n",
    "                    element_type in str(type(element)) for element_type in\n",
    "                    [\"unstructured.documents.elements.Image\"]\n",
    "            ):\n",
    "                element_category.images.append(element)\n",
    "            else:\n",
    "                print(f\"BaseDocumentProcessor.categorize_elements: Ignoring element type {type(element)}.\")\n",
    "\n",
    "        return element_category\n",
    "\n",
    "\n",
    "class SplitDocumentProcessor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def split_texts(self, texts: List[Text], chunk_size: int = 4000, chunk_overlap: int = 0) -> List[str]:\n",
    "        text_splitter: RecursiveCharacterTextSplitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap\n",
    "        )\n",
    "        text: str = \" \".join([text.text for text in texts])\n",
    "        splitted_text: List[str] = text_splitter.split_text(\n",
    "            text=text\n",
    "        )\n",
    "\n",
    "        return splitted_text\n",
    "\n",
    "\n",
    "class SummaryDocumentProcessor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def summarize_tables(self, tables: List[Table], model: BaseChatModel) -> List[str]:\n",
    "        prompt_text = \"\"\"You are an assistant tasked with summarizing tables for retrieval. \\\n",
    "        These summaries will be embedded and used to retrieve the tables. \\\n",
    "        Give a concise summary of the tables that is well optimized for retrieval. Table : {table} \"\"\"\n",
    "        prompt: ChatPromptTemplate = ChatPromptTemplate.from_template(prompt_text)\n",
    "        chain: RunnableSerializable = {\"table\": lambda table: table.text} | prompt | model | StrOutputParser()\n",
    "        summaries: List[str] = chain.batch(tables)\n",
    "\n",
    "        return summaries\n",
    "\n",
    "    def _get_message_from_image(self, model: BaseChatModel, prompt_text: str, image: Image) -> BaseMessage:\n",
    "        message = model.invoke([\n",
    "            HumanMessage(\n",
    "                content=[\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": prompt_text\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image\",\n",
    "                        \"source\": {\n",
    "                            \"type\": \"base64\",\n",
    "                            \"media_type\": image.metadata.image_mime_type,\n",
    "                            \"data\": image.metadata.image_base64\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "        ])\n",
    "\n",
    "        return message\n",
    "\n",
    "    def summarize_images(self, images: List[Image], model: BaseChatModel) -> List[str]:\n",
    "        prompt_text = \"\"\"You are an assistant tasked with summarizing images for retrieval. \\\n",
    "        These summaries will be embedded and used to retrieve the images. \\\n",
    "        Give a concise summary of the images that is well optimized for retrieval.\"\"\"\n",
    "        summaries: List[str] = []\n",
    "        for image in images:\n",
    "            message: BaseMessage = self._get_message_from_image(\n",
    "                model=model,\n",
    "                prompt_text=prompt_text,\n",
    "                image=image\n",
    "            )\n",
    "            summaries.append(message.content)\n",
    "\n",
    "        return summaries\n",
    "\n",
    "\n",
    "partition_document_processor: PartitionDocumentProcessor = PartitionDocumentProcessor(\n",
    "    document_management=document_management,\n",
    "    file_document_management=file_document_management,\n",
    "    text_document_management=text_document_management,\n",
    "    web_document_management=web_document_management,\n",
    ")\n",
    "\n",
    "category_document_processor: CategoryDocumentProcessor = CategoryDocumentProcessor()\n",
    "split_document_processor: SplitDocumentProcessor = SplitDocumentProcessor()\n",
    "summary_document_processor: SummaryDocumentProcessor = SummaryDocumentProcessor()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T05:25:22.015697Z",
     "start_time": "2024-03-31T05:25:22.003708Z"
    }
   },
   "id": "dd4988f2b8a6d69f",
   "execution_count": 71
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "elements: List[Element] = None\n",
    "\n",
    "\n",
    "async def handler(session: AsyncSession):\n",
    "    global elements\n",
    "    state: State = State()\n",
    "    state.session = session\n",
    "    state.authorized_session = all_seeder.session_seeder.session_mock.data[0]\n",
    "    elements = await partition_document_processor.partition(\n",
    "        state=state,\n",
    "        document_id=all_seeder.file_document_seeder.file_document_mock.data[0].id\n",
    "    )\n",
    "\n",
    "\n",
    "await one_datastore.retryable(handler)\n",
    "\n",
    "elements"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eeeb5ac45f572cf7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.Title'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.Header'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.FigureCaption'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.Header'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.Header'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.Header'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.Title'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.FigureCaption'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.Header'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.Header'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.Header'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.Header'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.Title'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.Header'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.Header'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.Header'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.Header'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.Header'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.Title'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.Header'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.Title'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.Header'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.FigureCaption'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.Header'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.Header'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.Header'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.Title'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.FigureCaption'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.Header'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.Header'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.Header'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.Header'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.Title'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.Header'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.Header'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.Header'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.Header'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.Header'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.Title'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.ListItem'>.\n",
      "BaseDocumentProcessor.categorize_elements: Ignoring element type <class 'unstructured.documents.elements.Header'>.\n"
     ]
    }
   ],
   "source": [
    "element_category: ElementCategory = await category_document_processor.categorize_elements(\n",
    "    elements=elements\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T05:26:06.162994Z",
     "start_time": "2024-03-31T05:26:06.160119Z"
    }
   },
   "id": "2b9c874c146e434a",
   "execution_count": 73
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import sentence_transformers python package. Please install it with `pip install sentence-transformers`.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/embeddings/huggingface.py:59\u001B[0m, in \u001B[0;36mHuggingFaceEmbeddings.__init__\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 59\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msentence_transformers\u001B[39;00m\n\u001B[1;32m     61\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'sentence_transformers'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[76], line 19\u001B[0m\n\u001B[1;32m     11\u001B[0m router \u001B[38;5;241m=\u001B[39m Router(model_list\u001B[38;5;241m=\u001B[39mmodel_list)\n\u001B[1;32m     12\u001B[0m llm \u001B[38;5;241m=\u001B[39m ChatLiteLLMRouter(\n\u001B[1;32m     13\u001B[0m     router\u001B[38;5;241m=\u001B[39mrouter,\n\u001B[1;32m     14\u001B[0m     model_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclaude-3-haiku\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     15\u001B[0m     streaming\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m     16\u001B[0m     temperature\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m,\n\u001B[1;32m     17\u001B[0m )\n\u001B[0;32m---> 19\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m \u001B[43mHuggingFaceE5InstructEmbeddings\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     20\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/mnt/c/Data/Apps/research-assistant-infrastructure/data/models/infloat/multilingual-e5-large-instruct\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     21\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdevice\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcuda\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     22\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencode_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mnormalize_embeddings\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     23\u001B[0m \u001B[43m    \u001B[49m\u001B[43mquery_instruction\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mGiven the question, retrieve the answer from the context.\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[1;32m     24\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/app/apps/inners/use_cases/embeddings/hugging_face_e5_instruct_embedding.py:10\u001B[0m, in \u001B[0;36mHuggingFaceE5InstructEmbeddings.__init__\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any):\n\u001B[0;32m---> 10\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/embeddings/huggingface.py:62\u001B[0m, in \u001B[0;36mHuggingFaceEmbeddings.__init__\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m     59\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msentence_transformers\u001B[39;00m\n\u001B[1;32m     61\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[0;32m---> 62\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[1;32m     63\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCould not import sentence_transformers python package. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     64\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease install it with `pip install sentence-transformers`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     65\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mexc\u001B[39;00m\n\u001B[1;32m     67\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient \u001B[38;5;241m=\u001B[39m sentence_transformers\u001B[38;5;241m.\u001B[39mSentenceTransformer(\n\u001B[1;32m     68\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_name, cache_folder\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcache_folder, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_kwargs\n\u001B[1;32m     69\u001B[0m )\n",
      "\u001B[0;31mImportError\u001B[0m: Could not import sentence_transformers python package. Please install it with `pip install sentence-transformers`."
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Could not import sentence_transformers python package. Please install it with `pip install sentence-transformers`.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/embeddings/huggingface.py:59\u001B[0m, in \u001B[0;36mHuggingFaceEmbeddings.__init__\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 59\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msentence_transformers\u001B[39;00m\n\u001B[1;32m     61\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'sentence_transformers'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[76], line 19\u001B[0m\n\u001B[1;32m     11\u001B[0m router \u001B[38;5;241m=\u001B[39m Router(model_list\u001B[38;5;241m=\u001B[39mmodel_list)\n\u001B[1;32m     12\u001B[0m llm \u001B[38;5;241m=\u001B[39m ChatLiteLLMRouter(\n\u001B[1;32m     13\u001B[0m     router\u001B[38;5;241m=\u001B[39mrouter,\n\u001B[1;32m     14\u001B[0m     model_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclaude-3-haiku\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     15\u001B[0m     streaming\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m     16\u001B[0m     temperature\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m,\n\u001B[1;32m     17\u001B[0m )\n\u001B[0;32m---> 19\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m \u001B[43mHuggingFaceE5InstructEmbeddings\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     20\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/mnt/c/Data/Apps/research-assistant-infrastructure/data/models/infloat/multilingual-e5-large-instruct\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     21\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdevice\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcuda\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     22\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencode_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mnormalize_embeddings\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     23\u001B[0m \u001B[43m    \u001B[49m\u001B[43mquery_instruction\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mGiven the question, retrieve the answer from the context.\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[1;32m     24\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/app/apps/inners/use_cases/embeddings/hugging_face_e5_instruct_embedding.py:10\u001B[0m, in \u001B[0;36mHuggingFaceE5InstructEmbeddings.__init__\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any):\n\u001B[0;32m---> 10\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/embeddings/huggingface.py:62\u001B[0m, in \u001B[0;36mHuggingFaceEmbeddings.__init__\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m     59\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msentence_transformers\u001B[39;00m\n\u001B[1;32m     61\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[0;32m---> 62\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[1;32m     63\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCould not import sentence_transformers python package. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     64\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease install it with `pip install sentence-transformers`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     65\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mexc\u001B[39;00m\n\u001B[1;32m     67\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient \u001B[38;5;241m=\u001B[39m sentence_transformers\u001B[38;5;241m.\u001B[39mSentenceTransformer(\n\u001B[1;32m     68\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_name, cache_folder\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcache_folder, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_kwargs\n\u001B[1;32m     69\u001B[0m )\n",
      "\u001B[0;31mImportError\u001B[0m: Could not import sentence_transformers python package. Please install it with `pip install sentence-transformers`."
     ]
    }
   ],
   "source": [
    "litellm.set_verbose = False\n",
    "model_list: List[Dict] = [\n",
    "    {\n",
    "        \"model_name\": \"claude-3-haiku\",\n",
    "        \"litellm_params\": {\n",
    "            \"model\": \"claude-3-haiku-20240307\",\n",
    "            \"api_key\": os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    "        }\n",
    "    }\n",
    "]\n",
    "router: Router = Router(model_list=model_list)\n",
    "model: ChatLiteLLMRouter = ChatLiteLLMRouter(\n",
    "    router=router,\n",
    "    model_name=\"claude-3-haiku\",\n",
    "    streaming=True,\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "embeddings: HuggingFaceE5InstructEmbeddings = HuggingFaceE5InstructEmbeddings(\n",
    "    model_name=\"/mnt/c/Data/Apps/research-assistant-infrastructure/data/models/infloat/multilingual-e5-large-instruct\",\n",
    "    model_kwargs={'device': 'cuda'},\n",
    "    encode_kwargs={'normalize_embeddings': True},\n",
    "    query_instruction=\"Given the question, retrieve the answer from the context.\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T05:33:18.868136Z",
     "start_time": "2024-03-31T05:33:18.807100Z"
    }
   },
   "id": "d11e68d03913b499",
   "execution_count": 76
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "summarized_tables: List[str] = summary_document_processor.summarize_tables(\n",
    "    tables=element_categories.tables,\n",
    "    model=model\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "656aad0b6701741"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "summarized_images: List[str] = summary_document_processor.summarize_images(\n",
    "    images=element_categories.images,\n",
    "    model=model\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d57937f65a50bcad"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "splitted_texts: List[str] = split_document_processor.split_texts(\n",
    "    chunk_size=10,\n",
    "    chunk_overlap=0,\n",
    "    texts=element_categories.texts\n",
    ")\n",
    "len(splitted_texts)\n",
    "splitted_texts[0], splitted_texts[1]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3b9f8f3fe59c3c8",
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class MultiVectorRetriever:\n",
    "    def __init__(\n",
    "            self,\n",
    "            two_datastore: TwoDatastore,\n",
    "            four_datastore: FourDatastore,\n",
    "    ):\n",
    "        self.two_datastore = two_datastore\n",
    "        self.four_datastore = four_datastore\n",
    "\n",
    "    def get_retriever(\n",
    "            self,\n",
    "            embedding_function: Embeddings,\n",
    "            collection_name: str,\n",
    "            **kwargs: Any\n",
    "    ) -> retrievers.MultiVectorRetriever:\n",
    "        document_store: RedisStore = RedisStore(\n",
    "            client=self.two_datastore.client,\n",
    "        )\n",
    "        vector_store: Milvus = self.four_datastore.get_client(\n",
    "            embedding_function=embedding_function,\n",
    "            collection_name=collection_name,\n",
    "        )\n",
    "        return retrievers.MultiVectorRetriever(\n",
    "            docstore=document_store,\n",
    "            vectorstore=vector_store,\n",
    "            **kwargs\n",
    "        )\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e7463967e7a6957"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class GraphState(TypedDict):\n",
    "    data: Dict[str, Any]\n",
    "\n",
    "\n",
    "class GraphLongFormQa:\n",
    "    def __init__(\n",
    "            self,\n",
    "            four_datastore: FourDatastore,\n",
    "    ):\n",
    "        self.four_datastore = four_datastore\n",
    "\n",
    "    def _retrieve(self, input_state: GraphState) -> GraphState:\n",
    "        output_state: GraphState = copy.deepcopy(input_state)\n",
    "        retriever: Milvus = self.four_datastore.get_client(\n",
    "            embedding_function=input_state[\"retriever_setting\"][\"embedding_function\"],\n",
    "            collection_name=input_state[\"retriever_setting\"][\"collection_name\"],\n",
    "        )\n",
    "        return output_state\n",
    "\n",
    "    def _get_node(self, node: RunnableLike) -> Tuple[str, RunnableLike]:\n",
    "        return node.__name__, node\n",
    "\n",
    "    def compile(self) -> CompiledGraph:\n",
    "        graph: StateGraph = StateGraph(GraphState)\n",
    "        graph.add_node(*self._get_node(self._retrieve))\n",
    "        graph.set_entry_point(self._retrieve.__name__)\n",
    "        graph.set_finish_point(self._retrieve.__name__)\n",
    "        compiled_graph: CompiledGraph = graph.compile()\n",
    "        return compiled_graph\n",
    "\n",
    "\n",
    "graph_lfqa = GraphLongFormQa(\n",
    "    four_datastore=four_datastore,\n",
    ")\n",
    "compiled_graph_lfqa = graph_lfqa.compile()\n",
    "input_state: GraphState = GraphState(\n",
    "    data={\n",
    "        \"retriever_setting\": {\n",
    "            \"embedding_function\": HuggingFaceE5InstructEmbeddings,\n",
    "            \"collection_name\": all_seeder.account_seeder.account_mock.data[0].id,\n",
    "        },\n",
    "        \"question\": \"what is artificial intelligence?\"\n",
    "    }\n",
    ")\n",
    "compiled_graph_lfqa.invoke(input_state)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ec7c6cbe7308833",
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['question', 'ground_truth', 'answer', 'contexts'],\n    num_rows: 1\n})"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_data = amnesty_qa[\"eval\"].select(range(1))\n",
    "eval_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T01:57:14.046695Z",
     "start_time": "2024-03-22T01:57:14.042355Z"
    }
   },
   "id": "13d40d240aad0178",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f2f7b33f294949b5bb0347497ba25c4e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid JSON response. Expected dictionary with key 'Attributed'\n",
      "/home/muazhari/miniconda3/envs/research-assistant/lib/python3.11/site-packages/ragas/evaluation.py:276: RuntimeWarning: Mean of empty slice\n",
      "  value = np.nanmean(self.scores[cn])\n"
     ]
    }
   ],
   "source": [
    "result = evaluate(\n",
    "    eval_data,\n",
    "    llm=llm,\n",
    "    embeddings=embeddings,\n",
    "    # metrics=[\n",
    "    #     metrics.faithfulness,\n",
    "    #     metrics.answer_relevancy, \n",
    "    #     metrics.context_recall,\n",
    "    #     metrics.context_precision,\n",
    "    #     metrics.answer_correctness,\n",
    "    #     metrics.context_relevancy,\n",
    "    #     metrics.context_entity_recall,\n",
    "    # ],\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T01:54:56.999815Z",
     "start_time": "2024-03-22T01:54:46.984340Z"
    }
   },
   "id": "994fa6d782998e85",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'answer_relevancy': 0.9599, 'context_precision': 1.0000, 'faithfulness': 0.5000, 'context_recall': nan}"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T01:55:02.560943Z",
     "start_time": "2024-03-22T01:55:02.557969Z"
    }
   },
   "id": "a74f0707bbbb234c",
   "execution_count": 22
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
