{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import base64\n",
    "import hashlib\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "import numpy\n",
    "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "os.chdir(\"/app\")\n",
    "\n",
    "from fastapi.encoders import jsonable_encoder\n",
    "from langchain.retrievers import MultiVectorRetriever\n",
    "from pymilvus.orm import utility\n",
    "from unstructured.partition.html import partition_html\n",
    "from unstructured.partition.text import partition_text\n",
    "\n",
    "from tools import dict_tool\n",
    "\n",
    "import gc\n",
    "\n",
    "from tools.cache_tool import cacher\n",
    "from apps.inners.exceptions import use_case_exception\n",
    "from apps.inners.models.dtos.document_category import DocumentCategory\n",
    "\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "from apps.outers.settings.one_llm_setting import OneLlmSetting\n",
    "from langchain_community.storage.redis import RedisStore\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from langchain_core.messages import BaseMessage, ChatMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document as LangChainDocument\n",
    "from litellm import Router\n",
    "\n",
    "from apps.inners.models.dtos.element_category import ElementCategory\n",
    "from uuid import UUID\n",
    "\n",
    "from sqlmodel.ext.asyncio.session import AsyncSession\n",
    "from starlette.datastructures import State\n",
    "\n",
    "from apps.inners.models.daos.document import Document\n",
    "from apps.inners.models.dtos.contracts.responses.managements.documents.file_document_response import \\\n",
    "    FileDocumentResponse\n",
    "from apps.inners.models.dtos.contracts.responses.managements.documents.text_document_response import \\\n",
    "    TextDocumentResponse\n",
    "from apps.inners.models.dtos.contracts.responses.managements.documents.web_document_response import WebDocumentResponse\n",
    "from apps.inners.use_cases.managements.document_management import DocumentManagement\n",
    "from apps.inners.use_cases.managements.file_document_management import FileDocumentManagement\n",
    "from apps.inners.use_cases.managements.text_document_management import TextDocumentManagement\n",
    "from apps.inners.use_cases.managements.web_document_management import WebDocumentManagement\n",
    "from typing import List, TypedDict\n",
    "from typing import Tuple, Dict, Any\n",
    "\n",
    "import dotenv\n",
    "from datasets import load_dataset\n",
    "from dotenv import find_dotenv\n",
    "from langchain_community.chat_models import ChatLiteLLMRouter\n",
    "from langchain_community.vectorstores.milvus import Milvus\n",
    "from langchain_core.runnables.base import RunnableSerializable\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.graph import CompiledGraph\n",
    "from ragas import evaluate\n",
    "from unstructured.documents.elements import Element, Table, Image, Text\n",
    "from unstructured.partition.auto import partition\n",
    "from unstructured.partition.utils.constants import PartitionStrategy\n",
    "\n",
    "from apps.inners.use_cases.embeddings.hugging_face_e5_instruct_embedding import HuggingFaceE5InstructEmbeddings\n",
    "from apps.outers.datastores.four_datastore import FourDatastore\n",
    "from apps.outers.datastores.one_datastore import OneDatastore\n",
    "from apps.outers.datastores.three_datastore import ThreeDatastore\n",
    "from apps.outers.datastores.two_datastore import TwoDatastore\n",
    "from apps.outers.repositories.file_document_repository import FileDocumentRepository\n",
    "from apps.outers.repositories.text_document_repository import TextDocumentRepository\n",
    "from apps.outers.repositories.web_document_repository import WebDocumentRepository\n",
    "from tests.containers.test_container import TestContainer\n",
    "from tests.seeders.all_seeder import AllSeeder\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T09:28:39.553589Z",
     "start_time": "2024-04-02T09:28:33.035319Z"
    }
   },
   "id": "ccc0387222ff1e05",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'d': {'data': {'embedding': {'model_name': 'intfloat/multilingual-e5-large-instruct',\n    'query_instruction': 'Given the question, retrieve the answer from the context.'},\n   'x': {'y': '1'}}}}"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = {\"data\": {\"embedding\": {\"model_name\", \"query_instruction\"}}}\n",
    "# keys = {\"data\": {\"x\"}}\n",
    "\n",
    "d = {\n",
    "    \"data\": {\n",
    "        \"embedding\": {\n",
    "            \"model_name\": \"intfloat/multilingual-e5-large-instruct\",\n",
    "            \"query_instruction\": \"Given the question, retrieve the answer from the context.\"\n",
    "        },\n",
    "        \"x\": {\n",
    "            \"y\": 1\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "kwargs = {\n",
    "    \"d\": d\n",
    "}\n",
    "_kwargs_include_keys = [\"d\"]\n",
    "# _kwargs_include_keys: Set[Any] = set([])\n",
    "# _kwargs_include_keys = _kwargs_include_keys.union(set(kwargs.keys()))\n",
    "\n",
    "args = (1, 2, 3, d)\n",
    "dict_args: Dict[Any, Any] = {}\n",
    "for key, arg in enumerate(args):\n",
    "    dict_args[key] = arg\n",
    "\n",
    "_args_include_keys = [0, {3: {\"data\": {\"embedding\": [\"model_name\"]}}}]\n",
    "\n",
    "dict_tool.filter_by_keys(dict_args, _args_include_keys)\n",
    "\n",
    "x = dict_tool.filter_by_keys(kwargs, _kwargs_include_keys)\n",
    "dict_tool.replace_end_value_to_string(x)\n",
    "# kwargs\n",
    "# \n",
    "\n",
    "# cache_tool.clear_cache()\n",
    "# \n",
    "# \n",
    "# # @cacher(kwargs_include_keys=keys)\n",
    "# @cacher()\n",
    "# def testx(x=None):\n",
    "#     return x\n",
    "# \n",
    "# \n",
    "# class testc:\n",
    "#     def __init__(self):\n",
    "#         pass\n",
    "# \n",
    "#     @cacher(args_include_keys=[0, {1: {\"data\": [\"embedding\"]}}])\n",
    "#     def testx(self, x=None):\n",
    "#         return x\n",
    "# \n",
    "# \n",
    "# testc().testx(d)\n",
    "# \n",
    "# testx(x=d)\n",
    "# \n",
    "# cache_tool.get_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T09:28:39.571824Z",
     "start_time": "2024-04-02T09:28:39.566042Z"
    }
   },
   "id": "d333970bfb39551b",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# import tensorflow\n",
    "# \n",
    "# tensorflow.config.list_physical_devices('GPU')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T09:28:39.580710Z",
     "start_time": "2024-04-02T09:28:39.578667Z"
    }
   },
   "id": "790b046202087813",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T09:28:42.554991Z",
     "start_time": "2024-04-02T09:28:41.623819Z"
    }
   },
   "id": "8638b0a2a9038acf",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dotenv.load_dotenv(find_dotenv())\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T09:28:42.564760Z",
     "start_time": "2024-04-02T09:28:42.556169Z"
    }
   },
   "id": "f0a206c2ced40865",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test_container = TestContainer()\n",
    "\n",
    "one_llm_setting: OneLlmSetting = test_container.applications.settings.one_llm()\n",
    "\n",
    "one_datastore: OneDatastore = test_container.applications.datastores.one()\n",
    "two_datastore: TwoDatastore = test_container.applications.datastores.two()\n",
    "three_datastore: ThreeDatastore = test_container.applications.datastores.three()\n",
    "four_datastore: FourDatastore = test_container.applications.datastores.four()\n",
    "temp_datastore: ThreeDatastore = test_container.applications.datastores.temp()\n",
    "\n",
    "file_document_repository: FileDocumentRepository = test_container.applications.repositories.file_document()\n",
    "text_document_repository: TextDocumentRepository = test_container.applications.repositories.text_document()\n",
    "web_document_repository: WebDocumentRepository = test_container.applications.repositories.web_document()\n",
    "\n",
    "document_management: DocumentManagement = test_container.applications.use_cases.managements.document()\n",
    "file_document_management: FileDocumentManagement = test_container.applications.use_cases.managements.file_document()\n",
    "text_document_management: TextDocumentManagement = test_container.applications.use_cases.managements.text_document()\n",
    "web_document_management: WebDocumentManagement = test_container.applications.use_cases.managements.web_document()\n",
    "\n",
    "all_seeder: AllSeeder = test_container.seeders.all()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T09:28:42.630347Z",
     "start_time": "2024-04-02T09:28:42.566138Z"
    }
   },
   "id": "8eb9c2d30510de79",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "await all_seeder.up()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T09:28:43.124773Z",
     "start_time": "2024-04-02T09:28:42.631588Z"
    }
   },
   "id": "1cf8b24205467927",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "await all_seeder.down()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d18db5f9358936e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await two_datastore.client.set(\"test\", \"test\", ex=10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T21:50:48.726484Z",
     "start_time": "2024-03-22T21:50:48.718837Z"
    }
   },
   "id": "bf6d7b9a59665490",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"
     ]
    }
   ],
   "source": [
    "# loading the V2 dataset\n",
    "amnesty_qa = load_dataset(\"explodinggradients/amnesty_qa\", \"english_v2\", trust_remote_code=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T15:18:15.438417Z",
     "start_time": "2024-03-31T15:18:09.282654Z"
    }
   },
   "id": "ec00454c2bc21e24",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    eval: Dataset({\n        features: ['question', 'ground_truth', 'answer', 'contexts'],\n        num_rows: 20\n    })\n})"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amnesty_qa"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T15:18:15.442729Z",
     "start_time": "2024-03-31T15:18:15.439558Z"
    }
   },
   "id": "f11930c0068cc28f",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class MainDocumentProcessor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def split_texts(self, texts: List[Text], chunk_size: int, chunk_overlap: int) -> List[str]:\n",
    "        text_splitter: RecursiveCharacterTextSplitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap\n",
    "        )\n",
    "        text: str = \" \".join([text.text for text in texts])\n",
    "        splitted_text: List[str] = text_splitter.split_text(\n",
    "            text=text\n",
    "        )\n",
    "\n",
    "        return splitted_text\n",
    "\n",
    "\n",
    "class PartitionDocumentProcessor:\n",
    "    def __init__(\n",
    "            self,\n",
    "            document_management: DocumentManagement,\n",
    "            file_document_management: FileDocumentManagement,\n",
    "            text_document_management: TextDocumentManagement,\n",
    "            web_document_management: WebDocumentManagement,\n",
    "    ):\n",
    "        self.document_management = document_management\n",
    "        self.file_document_management = file_document_management\n",
    "        self.text_document_management = text_document_management\n",
    "        self.web_document_management = web_document_management\n",
    "\n",
    "    async def _partition_file(self, state: State, found_document: Document) -> List[Element]:\n",
    "        found_file_document: FileDocumentResponse = await self.file_document_management.find_one_by_id_with_authorization(\n",
    "            state=state,\n",
    "            id=found_document.id\n",
    "        )\n",
    "        file_data: bytes = self.file_document_management.file_document_repository.get_object_data(\n",
    "            object_name=found_file_document.file_name\n",
    "        )\n",
    "        extract_image_path: Path = self.file_document_management.file_document_repository.file_path / found_file_document.file_data_hash\n",
    "        extract_image_path.mkdir(exist_ok=True)\n",
    "        shutil.rmtree(extract_image_path)\n",
    "        elements: List[Element] = partition(\n",
    "            metadata_filename=found_file_document.file_name,\n",
    "            file=io.BytesIO(file_data),\n",
    "            extract_images_in_pdf=True,\n",
    "            extract_image_block_output_dir=str(extract_image_path),\n",
    "            strategy=PartitionStrategy.AUTO,\n",
    "            hi_res_model_name=\"yolox\"\n",
    "        )\n",
    "\n",
    "        return elements\n",
    "\n",
    "    async def _partition_text(self, state: State, found_document: Document) -> List[Element]:\n",
    "        found_text_document: TextDocumentResponse = await self.text_document_management.find_one_by_id_with_authorization(\n",
    "            state=state,\n",
    "            id=found_document.id\n",
    "        )\n",
    "        elements: List[Element] = partition_text(\n",
    "            text=found_text_document.text_content\n",
    "        )\n",
    "\n",
    "        return elements\n",
    "\n",
    "    async def _partition_web(self, state: State, found_document: Document) -> List[Element]:\n",
    "        found_web_document: WebDocumentResponse = await self.web_document_management.find_one_by_id_with_authorization(\n",
    "            state=state,\n",
    "            id=found_document.id\n",
    "        )\n",
    "        elements: List[Element] = partition_html(\n",
    "            url=found_web_document.web_url,\n",
    "            ssl_verify=False\n",
    "        )\n",
    "\n",
    "        return elements\n",
    "\n",
    "    async def partition(self, state: State, document_id: UUID) -> List[Element]:\n",
    "        found_document: Document = await self.document_management.find_one_by_id_with_authorization(\n",
    "            state=state,\n",
    "            id=document_id\n",
    "        )\n",
    "        if found_document.document_type_id == \"file\":\n",
    "            elements: List[Element] = await self._partition_file(\n",
    "                state=state,\n",
    "                found_document=found_document\n",
    "            )\n",
    "        elif found_document.document_type_id == \"text\":\n",
    "            elements: List[Element] = await self._partition_text(\n",
    "                state=state,\n",
    "                found_document=found_document\n",
    "            )\n",
    "        elif found_document.document_type_id == \"web\":\n",
    "            elements: List[Element] = await self._partition_web(\n",
    "                state=state,\n",
    "                found_document=found_document\n",
    "            )\n",
    "        else:\n",
    "            raise use_case_exception.DocumentTypeNotSupported()\n",
    "\n",
    "        return elements\n",
    "\n",
    "\n",
    "class SummaryDocumentProcessor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def summarize_tables(self, tables: List[Table], model: BaseChatModel) -> List[str]:\n",
    "        prompt_text = \"\"\"You are an assistant tasked with summarizing tables for retrieval. \\\n",
    "        These summaries will be embedded and used to retrieve the table. \\\n",
    "        Give a concise passage summary of the table that is well optimized for retrieval. \\\n",
    "        Make sure the output only the summary without re-explaining. \\\n",
    "        Table : {table} \"\"\"\n",
    "        prompt: ChatPromptTemplate = ChatPromptTemplate.from_template(prompt_text)\n",
    "        chain: RunnableSerializable = {\"table\": lambda table: table.text} | prompt | model | StrOutputParser()\n",
    "        summaries: List[str] = chain.batch(tables)\n",
    "\n",
    "        return summaries\n",
    "\n",
    "    def _get_message_from_image(self, model: BaseChatModel, prompt_text: str, image: Image) -> BaseMessage:\n",
    "        message: BaseMessage = model.invoke([\n",
    "            ChatMessage(\n",
    "                role=\"user\",\n",
    "                content=[\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": prompt_text\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image\",\n",
    "                        \"source\": {\n",
    "                            \"type\": \"base64\",\n",
    "                            \"media_type\": image.metadata.image_mime_type,\n",
    "                            \"data\": image.metadata.image_base64\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "        ])\n",
    "\n",
    "        return message\n",
    "\n",
    "    def summarize_images(self, images: List[Image], model: BaseChatModel) -> List[str]:\n",
    "        prompt_text = \"\"\"You are an assistant tasked with summarizing images for retrieval. \\\n",
    "        These summaries will be embedded and used to retrieve the image. \\\n",
    "        Give a concise passage summary of the image that is well optimized for retrieval. \\\n",
    "        Make sure the output only the summary without re-explaining. \\\n",
    "        \"\"\"\n",
    "        summaries: List[str] = []\n",
    "        for image in images:\n",
    "            message: BaseMessage = self._get_message_from_image(\n",
    "                model=model,\n",
    "                prompt_text=prompt_text,\n",
    "                image=image\n",
    "            )\n",
    "            summaries.append(message.content)\n",
    "\n",
    "        return summaries\n",
    "\n",
    "\n",
    "class CategoryDocumentProcessor:\n",
    "    def __init__(\n",
    "            self,\n",
    "            main_document_processor: MainDocumentProcessor,\n",
    "            summary_document_processor: SummaryDocumentProcessor,\n",
    "    ):\n",
    "        self.main_document_processor = main_document_processor\n",
    "        self.summary_document_processor = summary_document_processor\n",
    "\n",
    "    async def categorize_elements(self, elements: List[Element]) -> ElementCategory:\n",
    "        categorized_elements: ElementCategory = ElementCategory(\n",
    "            texts=[],\n",
    "            tables=[],\n",
    "            images=[]\n",
    "        )\n",
    "\n",
    "        for element in elements:\n",
    "            if any(\n",
    "                    element_type in str(type(element)) for element_type in\n",
    "                    [\"unstructured.documents.elements.Text\", \"unstructured.documents.elements.NarrativeText\"]\n",
    "            ):\n",
    "                categorized_elements.texts.append(element)\n",
    "            elif any(\n",
    "                    element_type in str(type(element)) for element_type in\n",
    "                    [\"unstructured.documents.elements.Table\"]\n",
    "            ):\n",
    "                categorized_elements.tables.append(element)\n",
    "            elif any(\n",
    "                    element_type in str(type(element)) for element_type in\n",
    "                    [\"unstructured.documents.elements.Image\"]\n",
    "            ):\n",
    "                file_io = open(element.metadata.image_path, \"rb\")\n",
    "                element.metadata.image_mime_type = \"image/jpeg\"\n",
    "                element.metadata.image_base64 = base64.b64encode(file_io.read()).decode(\"utf-8\")\n",
    "                file_io.close()\n",
    "                categorized_elements.images.append(element)\n",
    "            else:\n",
    "                print(f\"BaseDocumentProcessor.categorize_elements: Ignoring element type {type(element)}.\")\n",
    "\n",
    "        return categorized_elements\n",
    "\n",
    "    def get_categorized_documents(\n",
    "            self,\n",
    "            categorized_elements: ElementCategory,\n",
    "            summarization_model: BaseChatModel,\n",
    "            is_include_tables: bool = False,\n",
    "            is_include_images: bool = False,\n",
    "            chunk_size: int = 400,\n",
    "            chunk_overlap: int = int(400 * 0.1),\n",
    "            id_key: str = \"id\"\n",
    "    ) -> DocumentCategory:\n",
    "        document_category: DocumentCategory = DocumentCategory(\n",
    "            texts=[],\n",
    "            tables=[],\n",
    "            images=[],\n",
    "            id_key=id_key\n",
    "        )\n",
    "        splitted_texts: List[str] = self.main_document_processor.split_texts(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            texts=categorized_elements.texts\n",
    "        )\n",
    "        for text in splitted_texts:\n",
    "            document_category.texts.append(LangChainDocument(\n",
    "                page_content=text,\n",
    "                metadata={\n",
    "                    id_key: str(uuid.uuid4())\n",
    "                }\n",
    "            ))\n",
    "\n",
    "        if is_include_tables:\n",
    "            summarized_tables: List[str] = self.summary_document_processor.summarize_tables(\n",
    "                tables=categorized_elements.tables,\n",
    "                model=summarization_model\n",
    "            )\n",
    "            for table in summarized_tables:\n",
    "                document_category.tables.append(LangChainDocument(\n",
    "                    page_content=table,\n",
    "                    metadata={\n",
    "                        id_key: str(uuid.uuid4())\n",
    "                    }\n",
    "                ))\n",
    "\n",
    "        if is_include_images:\n",
    "            summarized_images: List[str] = self.summary_document_processor.summarize_images(\n",
    "                images=categorized_elements.images,\n",
    "                model=summarization_model\n",
    "            )\n",
    "            for image, summarized_image in zip(categorized_elements.images, summarized_images):\n",
    "                document_category.images.append(LangChainDocument(\n",
    "                    page_content=summarized_image,\n",
    "                    metadata={\n",
    "                        id_key: str(uuid.uuid4()),\n",
    "                        \"image\": {\n",
    "                            \"mime_type\": image.metadata.image_mime_type,\n",
    "                            \"base64\": image.metadata.image_base64\n",
    "                        }\n",
    "                    }\n",
    "                ))\n",
    "\n",
    "        return document_category\n",
    "\n",
    "\n",
    "partition_document_processor: PartitionDocumentProcessor = PartitionDocumentProcessor(\n",
    "    document_management=document_management,\n",
    "    file_document_management=file_document_management,\n",
    "    text_document_management=text_document_management,\n",
    "    web_document_management=web_document_management,\n",
    ")\n",
    "\n",
    "main_document_processor: MainDocumentProcessor = MainDocumentProcessor()\n",
    "summary_document_processor: SummaryDocumentProcessor = SummaryDocumentProcessor()\n",
    "category_document_processor: CategoryDocumentProcessor = CategoryDocumentProcessor(\n",
    "    main_document_processor=main_document_processor,\n",
    "    summary_document_processor=summary_document_processor\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T09:28:43.958178Z",
     "start_time": "2024-04-02T09:28:43.943789Z"
    }
   },
   "id": "dd4988f2b8a6d69f",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class GraphState(TypedDict):\n",
    "    data: Dict[str, Any]\n",
    "\n",
    "\n",
    "class GraphLongFormQa:\n",
    "    def __init__(\n",
    "            self,\n",
    "            one_llm_setting: OneLlmSetting,\n",
    "            two_datastore: TwoDatastore,\n",
    "            four_datastore: FourDatastore,\n",
    "            category_document_processor: CategoryDocumentProcessor,\n",
    "    ):\n",
    "        self.one_llm_setting = one_llm_setting\n",
    "        self.two_datastore = two_datastore\n",
    "        self.four_datastore = four_datastore\n",
    "        self.category_document_processor = category_document_processor\n",
    "\n",
    "    def node_get_model(self, input_state: GraphState) -> GraphState:\n",
    "        output_state: GraphState = input_state\n",
    "        model_list: List[Dict] = [\n",
    "            {\n",
    "                \"model_name\": \"claude-3-haiku\",\n",
    "                \"litellm_params\": {\n",
    "                    \"model\": \"claude-3-haiku-20240307\",\n",
    "                    \"api_key\": self.one_llm_setting.LLM_ONE_ANTHROPIC_API_KEY_ONE,\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"model_name\": \"claude-3-opus\",\n",
    "                \"litellm_params\": {\n",
    "                    \"model\": \"claude-3-opus-20240229\",\n",
    "                    \"api_key\": self.one_llm_setting.LLM_ONE_ANTHROPIC_API_KEY_ONE,\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "        router: Router = Router(model_list=model_list)\n",
    "        model: ChatLiteLLMRouter = ChatLiteLLMRouter(\n",
    "            router=router,\n",
    "            model_name=input_state[\"data\"][\"llm\"][\"model_name\"],\n",
    "            streaming=True,\n",
    "            temperature=0,\n",
    "        )\n",
    "        output_state[\"data\"][\"llm\"][\"model\"] = model\n",
    "\n",
    "        return output_state\n",
    "\n",
    "    @cacher(args_include_keys=[], kwargs_include_keys=[\"model_name\"])\n",
    "    def _get_embedding_model(self, model_name: str) -> Embeddings:\n",
    "        if model_name == \"intfloat/multilingual-e5-large-instruct\":\n",
    "            model: HuggingFaceEmbeddings = HuggingFaceEmbeddings(\n",
    "                model_name=model_name,\n",
    "                model_kwargs={'device': 'cuda'},\n",
    "                encode_kwargs={'normalize_embeddings': True},\n",
    "            )\n",
    "        else:\n",
    "            raise use_case_exception.EmbeddingModelNameNotSupported()\n",
    "\n",
    "        return model\n",
    "\n",
    "    def node_get_embeddings(self, input_state: GraphState) -> GraphState:\n",
    "        output_state: GraphState = input_state\n",
    "        model: Embeddings = self._get_embedding_model(\n",
    "            model_name=input_state[\"data\"][\"embedding\"][\"model_name\"]\n",
    "        )\n",
    "\n",
    "        output_state[\"data\"][\"embedding\"][\"model\"] = model\n",
    "\n",
    "        return output_state\n",
    "\n",
    "    async def node_get_categorized_documents(self, input_state: GraphState) -> GraphState:\n",
    "        output_state: GraphState = input_state\n",
    "        document_id: UUID = input_state[\"data\"][\"document_id\"]\n",
    "\n",
    "        categorized_document_hash: str = self._get_categorized_document_hash(\n",
    "            document_id=document_id,\n",
    "            preprocessor_setting=input_state[\"data\"][\"preprocessor_setting\"]\n",
    "        )\n",
    "        input_state[\"data\"][\"categorized_document_hash\"] = categorized_document_hash\n",
    "        existing_categorized_document_hash: int = await self.two_datastore.client.exists(categorized_document_hash)\n",
    "        if existing_categorized_document_hash == 0:\n",
    "            is_categorized_document_exist: bool = False\n",
    "        elif existing_categorized_document_hash == 1:\n",
    "            is_categorized_document_exist: bool = True\n",
    "        else:\n",
    "            raise use_case_exception.ExistingCategorizedDocumentHashInvalid\n",
    "\n",
    "        if is_categorized_document_exist is False:\n",
    "            elements: List[Element] = await partition_document_processor.partition(\n",
    "                state=input_state[\"data\"][\"state\"],\n",
    "                document_id=document_id\n",
    "            )\n",
    "            categorized_elements: ElementCategory = await self.category_document_processor.categorize_elements(\n",
    "                elements=elements\n",
    "            )\n",
    "            categorized_documents: DocumentCategory = self.category_document_processor.get_categorized_documents(\n",
    "                categorized_elements=categorized_elements,\n",
    "                summarization_model=input_state[\"data\"][\"llm\"][\"model\"],\n",
    "                is_include_tables=input_state[\"data\"][\"preprocessor_setting\"][\"is_include_tables\"],\n",
    "                is_include_images=input_state[\"data\"][\"preprocessor_setting\"][\"is_include_images\"],\n",
    "                chunk_size=input_state[\"data\"][\"preprocessor_setting\"][\"chunk_size\"],\n",
    "                chunk_overlap=input_state[\"data\"][\"preprocessor_setting\"][\"chunk_overlap\"],\n",
    "            )\n",
    "            await self.two_datastore.client.set(\n",
    "                name=categorized_document_hash,\n",
    "                value=json.dumps(categorized_documents.dict(), default=jsonable_encoder)\n",
    "            )\n",
    "        else:\n",
    "            found_categorized_document_bytes: bytes = await self.two_datastore.client.get(categorized_document_hash)\n",
    "            categorized_documents: DocumentCategory = DocumentCategory(**json.loads(found_categorized_document_bytes))\n",
    "\n",
    "        output_state[\"data\"][\"categorized_documents\"] = categorized_documents\n",
    "\n",
    "        return output_state\n",
    "\n",
    "    def _get_categorized_document_hash(self, document_id: UUID, preprocessor_setting: Dict[str, Any]) -> str:\n",
    "        data: Dict[str, Any] = {\n",
    "            \"document_id\": document_id,\n",
    "            \"preprocessor_setting\": preprocessor_setting,\n",
    "        }\n",
    "        hashed_data: str = hashlib.sha256(\n",
    "            string=json.dumps(data, sort_keys=True, default=jsonable_encoder).encode()\n",
    "        ).hexdigest()\n",
    "\n",
    "        return hashed_data\n",
    "\n",
    "    def _get_collection_name_hash(self, categorized_document_hash: str, embedding_model_name: str,\n",
    "                                  prefix: str = \"lfqa\") -> str:\n",
    "        data: Dict[str, Any] = {\n",
    "            \"categorized_document_hash\": categorized_document_hash,\n",
    "            \"embedding_model_name\": embedding_model_name,\n",
    "        }\n",
    "        hashed_data: str = hashlib.sha256(\n",
    "            string=json.dumps(data, sort_keys=True, default=jsonable_encoder).encode()\n",
    "        ).hexdigest()\n",
    "        collection_name: str = f\"{prefix}_{hashed_data}\"\n",
    "\n",
    "        return collection_name\n",
    "\n",
    "    async def node_retrieve(self, input_state: GraphState) -> GraphState:\n",
    "        output_state: GraphState = input_state\n",
    "        categorized_documents: DocumentCategory = input_state[\"data\"][\"categorized_documents\"]\n",
    "        documents: List[LangChainDocument] = (\n",
    "                categorized_documents.texts +\n",
    "                categorized_documents.tables +\n",
    "                categorized_documents.images\n",
    "        )\n",
    "        document_contents: List[str] = []\n",
    "        document_meta_datas: List[Dict[str, Any]] = []\n",
    "        document_ids: List[str] = []\n",
    "        document_key_value_pairs: List[Tuple[Any, Any]] = []\n",
    "        for document in documents:\n",
    "            document_contents.append(document.page_content)\n",
    "            document_meta_datas.append(document.metadata)\n",
    "            document_ids.append(document.metadata[categorized_documents.id_key])\n",
    "            document_key_value_pairs.append(\n",
    "                (document.metadata[categorized_documents.id_key],\n",
    "                 bytes(json.dumps(document.dict(), default=jsonable_encoder).encode()))\n",
    "            )\n",
    "\n",
    "        embedding_model: Embeddings = input_state[\"data\"][\"embedding\"][\"model\"]\n",
    "        collection_name: str = self._get_collection_name_hash(\n",
    "            categorized_document_hash=input_state[\"data\"][\"categorized_document_hash\"],\n",
    "            embedding_model_name=input_state[\"data\"][\"embedding\"][\"model_name\"]\n",
    "        )\n",
    "        document_store: RedisStore = RedisStore(\n",
    "            redis_url=self.two_datastore.two_datastore_setting.URL,\n",
    "        )\n",
    "        vector_store: Milvus = self.four_datastore.get_client(\n",
    "            embedding_function=embedding_model,\n",
    "            collection_name=collection_name,\n",
    "        )\n",
    "        retriever: MultiVectorRetriever = MultiVectorRetriever(\n",
    "            vectorstore=vector_store,\n",
    "            docstore=document_store,\n",
    "            collection_name=collection_name,\n",
    "            id_key=input_state[\"data\"][\"categorized_documents\"].id_key,\n",
    "            search_kwargs={\n",
    "                \"k\": input_state[\"data\"][\"retriever_setting\"][\"top_k\"]\n",
    "            }\n",
    "        )\n",
    "\n",
    "        is_collection_exists: bool = utility.has_collection(collection_name, using=vector_store.alias)\n",
    "        is_force_refresh_embedding: bool = input_state[\"data\"][\"retriever_setting\"][\"is_force_refresh_embedding\"]\n",
    "        if is_collection_exists is False or is_force_refresh_embedding is True:\n",
    "            utility.drop_collection(collection_name, using=vector_store.alias)\n",
    "            await retriever.vectorstore.aadd_texts(\n",
    "                texts=document_contents,\n",
    "                metadatas=document_meta_datas,\n",
    "                ids=document_ids\n",
    "            )\n",
    "            await retriever.docstore.amset(key_value_pairs=document_key_value_pairs)\n",
    "\n",
    "        query: str = HuggingFaceE5InstructEmbeddings.get_detailed_instruct(\n",
    "            task_description=input_state[\"data\"][\"embedding\"][\"query_instruction\"],\n",
    "            query=input_state[\"data\"][\"question\"]\n",
    "        )\n",
    "        vector_store_retrieved_documents: List[\n",
    "            Tuple[LangChainDocument, float]\n",
    "        ] = await retriever.vectorstore.asimilarity_search_with_score(\n",
    "            query=query,\n",
    "            **retriever.search_kwargs\n",
    "        )\n",
    "\n",
    "        vector_store_retrieved_document_ids: List[str] = []\n",
    "        for vector_store_retrieved_document in vector_store_retrieved_documents:\n",
    "            vector_store_retrieved_document_ids.append(\n",
    "                vector_store_retrieved_document[0].metadata[categorized_documents.id_key])\n",
    "\n",
    "        doc_store_retrieved_documents: List[LangChainDocument | None] = await retriever.docstore.amget(\n",
    "            keys=vector_store_retrieved_document_ids\n",
    "        )\n",
    "\n",
    "        decoded_retrieved_documents: List[LangChainDocument] = []\n",
    "        for vector_store_retrieved_document, doc_store_retrieved_documents in zip(\n",
    "                vector_store_retrieved_documents, doc_store_retrieved_documents\n",
    "        ):\n",
    "            decoded_retrieved_document: LangChainDocument = LangChainDocument(\n",
    "                **json.loads(doc_store_retrieved_documents.decode())\n",
    "            )\n",
    "            decoded_retrieved_document.metadata[\"score\"] = vector_store_retrieved_document[1]\n",
    "            decoded_retrieved_documents.append(decoded_retrieved_document)\n",
    "\n",
    "        decoded_retrieved_documents.sort(\n",
    "            key=lambda x: x.metadata[\"score\"],\n",
    "            reverse=True\n",
    "        )\n",
    "        output_state[\"data\"][\"retrieved_documents\"] = decoded_retrieved_documents\n",
    "\n",
    "        return output_state\n",
    "\n",
    "    def compile(self) -> CompiledGraph:\n",
    "        graph: StateGraph = StateGraph(GraphState)\n",
    "\n",
    "        graph.add_node(self.node_get_model.__name__, self.node_get_model)\n",
    "        graph.add_node(self.node_get_embeddings.__name__, self.node_get_embeddings)\n",
    "        graph.add_node(self.node_get_categorized_documents.__name__, self.node_get_categorized_documents)\n",
    "        graph.add_node(self.node_retrieve.__name__, self.node_retrieve)\n",
    "\n",
    "        graph.set_entry_point(self.node_get_model.__name__)\n",
    "\n",
    "        graph.add_edge(self.node_get_model.__name__, self.node_get_embeddings.__name__)\n",
    "        graph.add_edge(self.node_get_embeddings.__name__, self.node_get_categorized_documents.__name__)\n",
    "        graph.add_edge(self.node_get_categorized_documents.__name__, self.node_retrieve.__name__)\n",
    "\n",
    "        graph.set_finish_point(self.node_retrieve.__name__)\n",
    "\n",
    "        compiled_graph: CompiledGraph = graph.compile()\n",
    "\n",
    "        return compiled_graph\n",
    "\n",
    "\n",
    "output_state: GraphState\n",
    "\n",
    "\n",
    "async def handler(session: AsyncSession):\n",
    "    global output_state\n",
    "\n",
    "    state: State = State()\n",
    "    state.authorized_session = all_seeder.session_seeder.session_mock.data[0]\n",
    "    state.session = session\n",
    "\n",
    "    graph_lfqa = GraphLongFormQa(\n",
    "        one_llm_setting=one_llm_setting,\n",
    "        two_datastore=two_datastore,\n",
    "        four_datastore=four_datastore,\n",
    "        category_document_processor=category_document_processor\n",
    "    )\n",
    "    compiled_graph_lfqa = graph_lfqa.compile()\n",
    "\n",
    "    data: Dict[str, Any] = {\n",
    "        \"state\": state,\n",
    "        \"document_id\": all_seeder.file_document_seeder.file_document_mock.data[0].id,\n",
    "        \"llm\": {\n",
    "            \"model_name\": \"claude-3-haiku\"\n",
    "        },\n",
    "        \"embedding\": {\n",
    "            \"model_name\": \"intfloat/multilingual-e5-large-instruct\",\n",
    "            \"query_instruction\": \"Given the question, retrieve the answer from the context.\",\n",
    "        },\n",
    "        \"preprocessor_setting\": {\n",
    "            \"chunk_size\": 50,\n",
    "            \"chunk_overlap\": numpy.floor(50 * 0.1),\n",
    "            \"is_include_tables\": False,\n",
    "            \"is_include_images\": False,\n",
    "        },\n",
    "        \"retriever_setting\": {\n",
    "            \"top_k\": 3,\n",
    "            \"is_force_refresh_embedding\": False,\n",
    "        },\n",
    "        \"question\": \"what is lorem ipsum?\",\n",
    "    }\n",
    "\n",
    "    input_state: GraphState = GraphState(\n",
    "        data=data\n",
    "    )\n",
    "    output_state = await compiled_graph_lfqa.ainvoke(input_state)\n",
    "\n",
    "\n",
    "await one_datastore.retryable(handler)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-04-02T09:34:17.749018Z"
    }
   },
   "id": "9ec7c6cbe7308833",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'data': {'state': <starlette.datastructures.State at 0x7f181578aad0>,\n  'document_id': UUID('0c92d4aa-50c5-44a3-9959-fac3bca6d15f'),\n  'llm': {'model_name': 'claude-3-haiku',\n   'model': ChatLiteLLMRouter(client=<module 'litellm' from '/usr/local/lib/python3.10/dist-packages/litellm/__init__.py'>, model_name='claude-3-haiku', openai_api_key='', azure_api_key='', anthropic_api_key='', replicate_api_key='', cohere_api_key='', openrouter_api_key='', streaming=True, temperature=0.0, router=<litellm.router.Router object at 0x7f17aefdff10>, huggingface_api_key='', together_ai_api_key='')},\n  'embedding': {'model_name': 'intfloat/multilingual-e5-large-instruct',\n   'query_instruction': 'Given the question, retrieve the answer from the context.',\n   'model': HuggingFaceEmbeddings(client=SentenceTransformer(\n     (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: XLMRobertaModel \n     (1): Pooling({'word_embedding_dimension': 1024, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n     (2): Normalize()\n   ), model_name='intfloat/multilingual-e5-large-instruct', cache_folder=None, model_kwargs={'device': 'cuda'}, encode_kwargs={'normalize_embeddings': True}, multi_process=False, show_progress=False)},\n  'preprocessor_setting': {'chunk_size': 50,\n   'chunk_overlap': 5.0,\n   'is_include_tables': False,\n   'is_include_images': False},\n  'retriever_setting': {'top_k': 3, 'is_force_refresh_embedding': False},\n  'question': 'what is lorem ipsum?',\n  'categorized_document_hash': 'b736058aee73ab746b99e312039610aef7ab55d2c43985511267efe4eb1aaba2',\n  'categorized_documents': DocumentCategory(texts=[Document(page_content='In publishing and graphic design, Lorem ipsum is a placeholder text commonly used to demonstrate the visual form of a documents or a typeface without relying on meaningful content. Lorem ipsum may be used as a placeholder before final copy is', metadata={'id': 'd8a5b28c-598e-4e37-a00a-60378311ebd6'}), Document(page_content='placeholder before final copy is available. It is also used to temporarily replace text in a process called greeking, which allows designers to consider the form of a webpage or publication, without the meaning of the text influencing the design. Lorem', metadata={'id': 'fd6604a4-c888-49cf-a9ab-14d09ac7b217'}), Document(page_content='the design. Lorem ipsum is typically a corrupted version of De finibus bonorum et malorum, a 1st-century BC text by the Roman statesman and philosopher Cicero, with words altered, added, and removed to make', metadata={'id': '00501abf-09ab-404e-aa6f-4fcda0f7f041'}), Document(page_content='and removed to make it nonsensical and improper Latin. Versions of the Lorem ipsum text have been used in typesetting at least since the 1960s, when it was popularized by advertisements for Letraset transfer sheets. [1]', metadata={'id': '1b9f31fc-98f8-41e1-b54a-485bd615dc5d'}), Document(page_content='sheets. [1] Lorem ipsum was introduced to the digital world in the mid-1980s, when Aldus employed it in graphic and word-processing templates for its desktop publishing program PageMaker. Other popular word processors, including Pages', metadata={'id': 'fd6c0392-0ddb-45a4-8ece-42eb671060c2'}), Document(page_content='word processors, including Pages and Microsoft Word, have since adopted Lorem ipsum,[2] as have many LaTeX packages,[3][4][5] web content managers such as Joomla! and WordPress, and CSS libraries such as', metadata={'id': 'a905f185-5ae7-4870-a5b1-c022d0c278f1'}), Document(page_content='and CSS libraries such as Semantic UI. [6]', metadata={'id': 'ef0accb0-665f-4e02-8aca-421f561da0b9'})], tables=[], images=[], id_key='id'),\n  'retrieved_documents': [Document(page_content='sheets. [1] Lorem ipsum was introduced to the digital world in the mid-1980s, when Aldus employed it in graphic and word-processing templates for its desktop publishing program PageMaker. Other popular word processors, including Pages', metadata={'id': 'fd6c0392-0ddb-45a4-8ece-42eb671060c2', 'score': 0.19670191407203674}),\n   Document(page_content='the design. Lorem ipsum is typically a corrupted version of De finibus bonorum et malorum, a 1st-century BC text by the Roman statesman and philosopher Cicero, with words altered, added, and removed to make', metadata={'id': '00501abf-09ab-404e-aa6f-4fcda0f7f041', 'score': 0.1857132613658905}),\n   Document(page_content='In publishing and graphic design, Lorem ipsum is a placeholder text commonly used to demonstrate the visual form of a documents or a typeface without relying on meaningful content. Lorem ipsum may be used as a placeholder before final copy is', metadata={'id': 'd8a5b28c-598e-4e37-a00a-60378311ebd6', 'score': 0.13832782208919525})]}}"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(output_state[\"data\"][\"categorized_documents\"].texts[0].page_content)\n",
    "output_state"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T09:34:07.760689Z",
     "start_time": "2024-04-02T09:34:07.756719Z"
    }
   },
   "id": "364b2c6821720c3b",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[Document(page_content='sheets. [1] Lorem ipsum was introduced to the digital world in the mid-1980s, when Aldus employed it in graphic and word-processing templates for its desktop publishing program PageMaker. Other popular word processors, including Pages', metadata={'id': 'fd6c0392-0ddb-45a4-8ece-42eb671060c2', 'score': 0.19670191407203674}),\n Document(page_content='the design. Lorem ipsum is typically a corrupted version of De finibus bonorum et malorum, a 1st-century BC text by the Roman statesman and philosopher Cicero, with words altered, added, and removed to make', metadata={'id': '00501abf-09ab-404e-aa6f-4fcda0f7f041', 'score': 0.1857132613658905}),\n Document(page_content='In publishing and graphic design, Lorem ipsum is a placeholder text commonly used to demonstrate the visual form of a documents or a typeface without relying on meaningful content. Lorem ipsum may be used as a placeholder before final copy is', metadata={'id': 'd8a5b28c-598e-4e37-a00a-60378311ebd6', 'score': 0.13832782208919525})]"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_state[\"data\"][\"retrieved_documents\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T09:29:35.417457Z",
     "start_time": "2024-04-02T09:29:35.413663Z"
    }
   },
   "id": "e75b843df6d2654f",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'amnesty_qa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[24], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m eval_data \u001B[38;5;241m=\u001B[39m \u001B[43mamnesty_qa\u001B[49m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meval\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m      2\u001B[0m eval_data\n",
      "\u001B[0;31mNameError\u001B[0m: name 'amnesty_qa' is not defined"
     ]
    }
   ],
   "source": [
    "eval_data = amnesty_qa[\"eval\"].select(range(1))\n",
    "eval_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T00:16:07.076096Z",
     "start_time": "2024-04-02T00:16:07.055636Z"
    }
   },
   "id": "13d40d240aad0178",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f2f7b33f294949b5bb0347497ba25c4e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001B[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid JSON response. Expected dictionary with key 'Attributed'\n",
      "/home/muazhari/miniconda3/envs/research-assistant/lib/python3.11/site-packages/ragas/evaluation.py:276: RuntimeWarning: Mean of empty slice\n",
      "  value = np.nanmean(self.scores[cn])\n"
     ]
    }
   ],
   "source": [
    "result = evaluate(\n",
    "    eval_data,\n",
    "    llm=llm,\n",
    "    embeddings=embeddings,\n",
    "    # metrics=[\n",
    "    #     metrics.faithfulness,\n",
    "    #     metrics.answer_relevancy, \n",
    "    #     metrics.context_recall,\n",
    "    #     metrics.context_precision,\n",
    "    #     metrics.answer_correctness,\n",
    "    #     metrics.context_relevancy,\n",
    "    #     metrics.context_entity_recall,\n",
    "    # ],\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T01:54:56.999815Z",
     "start_time": "2024-03-22T01:54:46.984340Z"
    }
   },
   "id": "994fa6d782998e85",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'answer_relevancy': 0.9599, 'context_precision': 1.0000, 'faithfulness': 0.5000, 'context_recall': nan}"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T01:55:02.560943Z",
     "start_time": "2024-03-22T01:55:02.557969Z"
    }
   },
   "id": "a74f0707bbbb234c",
   "execution_count": 22
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
